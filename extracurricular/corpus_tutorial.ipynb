{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOt+ezqk4J3CuWRE22PGuiw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vifirsanova/hse-python-course/blob/main/extracurricular/corpus_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Лицензия"
      ],
      "metadata": {
        "id": "QVTnnVtQ8jO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MIT License\n",
        "\n",
        "Copyright (c) 2023 Victoria Firsanova\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ],
      "metadata": {
        "id": "VSx3ap-H7czd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Импорт библиотек"
      ],
      "metadata": {
        "id": "c1Offge4klq7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VRPYOizkWgj"
      },
      "outputs": [],
      "source": [
        "from datetime import date, timedelta\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Генерация ссылок\n",
        "\n",
        "1. Генерация списка дат"
      ],
      "metadata": {
        "id": "2fUlrBl5kp-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dates(start_date, end_date):\n",
        "  \"\"\"\n",
        "  Функция принимает на вход даты для генерации списка дат, возвращает список дат.\n",
        "  start_date и end_date являются классами типа date из модуля datetime\n",
        "  start_date: date()\n",
        "  end_date: date()\n",
        "  \"\"\"\n",
        "\n",
        "  # список дат\n",
        "\n",
        "  dates = []\n",
        "\n",
        "  # с помощью timedelta мы задаем какую разницу во времени мы отсчитываем\n",
        "  # поскольку нас интересуют статьи за каждый день, мы отсчитываем 1 день от каждой заданной даты\n",
        "  # например, мы берем дату 24.07.2021 в качестве начальной точки отсчета\n",
        "  # затем мы используем дельту равную одному дню и получаем дату 25.07.2021\n",
        "\n",
        "  delta = timedelta(days = 1)\n",
        "\n",
        "  # цикл: до тех пор, пока первоначальная дата не совпадет с (меньше или равна) финальной\n",
        "  while start_date <= end_date:\n",
        "    # добавляем текущую (стартовую) дату в список дат\n",
        "    dates.append(start_date)\n",
        "    # используем timedelta: прибавляем 1 день к текущей (стартовой) дате\n",
        "    start_date += delta\n",
        "\n",
        "  # функция возвращает список дат\n",
        "\n",
        "  return dates"
      ],
      "metadata": {
        "id": "UaEFYvhfkbRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# объекты типа date оперирует датами в формате (год, месяц, день)\n",
        "# задаем даты\n",
        "\n",
        "start_date = date(###)\n",
        "end_date = date(###)\n",
        "\n",
        "# применяем нашу функцию и смотрим результат\n",
        "\n",
        "dates = generate_dates(start_date, end_date)\n",
        "dates[###]"
      ],
      "metadata": {
        "id": "BoSmj-Nnkt4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Генерация ссылок для запроса по архивам"
      ],
      "metadata": {
        "id": "nWIx90_ok2ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_links(website, dates):\n",
        "  \"\"\"\n",
        "  Функция для генерации ссылок. Принимает на вход названия новостных сайтов и\n",
        "  список ссылок.\n",
        "\n",
        "  website: str\n",
        "  dates: list\n",
        "  \"\"\"\n",
        "  # создаем список для хранения ссылок\n",
        "  links = []\n",
        "  # создаем временное хранилище для ссылок, которые мы будем записывать в список\n",
        "  temp = ''\n",
        "\n",
        "  # начинаем итерацию по всем элементам списка dates\n",
        "  for i in range(len(dates)):\n",
        "    # поспользуемся форматированием строки для компляции ссылок\n",
        "    # используем нашу структуру https://***.ru/news/yyyy/mm/dd\n",
        "    # форматированию подвергаются элементы ***, yyyy/mm/dd\n",
        "    # на место *** помещаем название сайта, записанное в переменную website\n",
        "    # на место yyyy/mm/dd помещаем значения из списка dates текущей итерации\n",
        "    # вспоминаем, как делать запрос к формату dates (см. ячейку выше)\n",
        "    # так, чтобы получить значение года, вводим dates[i].year, где\n",
        "    # > dates - обращение к списку дат\n",
        "    # > [i] - текущая итерация\n",
        "    # > year - извлечение года из формата date модуля datetime\n",
        "    year = dates[i].year\n",
        "    # к значению месяца нужно приписать 0 слева в том случае,\n",
        "    # если это значение меньше 10\n",
        "    # иначе мы не сможем сделать запросы к сайтам,\n",
        "    # потому что на сайтах используется формат вида 24-_0_7-2023\n",
        "    month = dates[i].month if dates[i].month > 9 else f'0{dates[i].month}'\n",
        "    # то же самое нужно сделать для дней\n",
        "    day = dates[i].day if dates[i].day > 9 else f'0{dates[i].day}'\n",
        "    temp = f'https://{website}.ru/news/{year}/{month}/{day}'\n",
        "    links.append(temp)\n",
        "  return links"
      ],
      "metadata": {
        "id": "xpSUCuOIkxrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# генерация списка ссылок\n",
        "# добавим название сайта и дату\n",
        "\n",
        "links = generate_links(###, ###)\n",
        "links[###]"
      ],
      "metadata": {
        "id": "y_NsGb2_k58c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Выгрузка архивов"
      ],
      "metadata": {
        "id": "Rh0HeSoBlIhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = ###\n",
        "r = requests.get(sample)\n",
        "r"
      ],
      "metadata": {
        "id": "bldCvq2KE5-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(r.content, 'html.parser')"
      ],
      "metadata": {
        "id": "L4xWR1L1FCD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup"
      ],
      "metadata": {
        "id": "7ugCU6fEFHDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json.loads(soup.find('script', type='application/ld+json').text)['headline']"
      ],
      "metadata": {
        "id": "e5c-ACLK2EVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scraping(links):\n",
        "  \"\"\"\n",
        "  Функция для скрейпинга веб-страниц.\n",
        "  Используется как для извлечения информации из архивов,\n",
        "  так и для извлечения информации с новостных страниц сайтов.\n",
        "\n",
        "  На вход принимается:\n",
        "  links: список ссылок (list)\n",
        "\n",
        "  Функция возвращает список собранных данных:\n",
        "  data: list\n",
        "  \"\"\"\n",
        "  # создаем хранилище для собранной информации\n",
        "  data = []\n",
        "  # извлекаем ссылку из нашего списка с помощью итерирования\n",
        "  for link in links:\n",
        "    try:\n",
        "      # делаем запрос к странице с помощью requests.get\n",
        "      r = requests.get(link)\n",
        "      # проверяем успешность запроса\n",
        "      status = r.status_code\n",
        "      # если запрос успешный, то есть возвращается значение 200\n",
        "      if status == 200:\n",
        "        # производим парсинг страницы с помощью HTML-парсера BeautifulSoup\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "        # сохраняем в хранилище\n",
        "        data.append(soup)\n",
        "        # для дебагинга кода будем возвращать статус загрузки\n",
        "        print(f'Успешно выгружены данные со страницы {link}')\n",
        "      # если значение запроса иное, например, 404\n",
        "      else:\n",
        "        # возвращаем сообщение об ошибке\n",
        "        print(f'При попытке запроса произошла ошибка. Код {r.status_code}')\n",
        "    # прописываем действия на тот случай, если сделать запрос не удалось\n",
        "    except requests.exceptions.RequestException as e:\n",
        "      # возвращаем сообщение об ошибке\n",
        "      print(f'При попытке запроса произошла ошибка {e}')\n",
        "  # при успешном прохождении всех ссылок, получаем наши данные\n",
        "  return data"
      ],
      "metadata": {
        "id": "UTsoDyJMk-B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# последовательно применяем нашу функцию к ссылкам\n",
        "\n",
        "archive = scraping(links)"
      ],
      "metadata": {
        "id": "p5paEI6KlMyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Извлечение новостных страниц из архивов"
      ],
      "metadata": {
        "id": "hoAbEoduleTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# применим ту же функцию для пары тестовых страниц\n",
        "\n",
        "###\n",
        "\n",
        "sample = scraping(###)"
      ],
      "metadata": {
        "id": "3KR28jMtlrSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# предварительно зададим функцию для формирования ссылок\n",
        "\n",
        "def form_href(element):\n",
        "  \"\"\"\n",
        "  Функция принимает на вход элемент списка\n",
        "  из временного хранилища информации с архива сайта lenta (BeautifulSoup).\n",
        "\n",
        "  Функция возвращает сформированную ссылку для скрейпинга (str).\n",
        "  \"\"\"\n",
        "  # извлекаем из элемента ссылку\n",
        "  # более подробно работа с парсингом описана выше\n",
        "  href = element['href'] # пример извлечения: /news/2021/07/24/***/\n",
        "  # собираем ссылку\n",
        "  # префикс 'https://lenta.ru' соединяем с извлеменной ссылкой\n",
        "  return 'https://lenta.ru' + href"
      ],
      "metadata": {
        "id": "XIlZPzLcuqS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_content(contents):\n",
        "  temp = json.loads(contents.find('script', type='application/ld+json').text)\n",
        "  content = {'headline': temp['headline'], 'description': temp['description'], 'body': temp['articleBody']}\n",
        "  return content"
      ],
      "metadata": {
        "id": "z9KAQWfO3rG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# создаем хранилище для новостного контента\n",
        "\n",
        "content = []"
      ],
      "metadata": {
        "id": "jHzkLMeV3i_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# проходим по каждому элементу архива,\n",
        "# то есть по данным за каждый день\n",
        "for page in archive:\n",
        "  # находим все элементы класса \"card-full-news _archive\"\n",
        "  # методом find_all из BeautifulSoup\n",
        "  # ранее мы выяснили, что именно в этом классе хранятся искомые ссылки\n",
        "  # в переменной temp хранится список (list) всех элементов класса \"card-full-news _archive\"\n",
        "  temp = page.find_all(\"a\", class_=\"card-full-news _archive\")\n",
        "  # проходим по всем элементам этого списка\n",
        "  # здесь используем созданную ранее функцию lenta_href для создания ссылок\n",
        "  # с помощью генератора списков создаем набор ссылок для каждого дня в архиве\n",
        "  temp_links = [form_href(element) for element in temp]\n",
        "  # используем созданную ранее функцию scraping для скрейпинга данных\n",
        "  # по этим ссылкам\n",
        "  contents = scraping(temp_links)\n",
        "  # добавляем извлеченную информацию в хранилище content\n",
        "  # contents хранит список данных за по каждой ссылке\n",
        "  # эти данные представляют собой объект BeautifulSoup\n",
        "  # с помощью get_content мы извлекаем текстовые данные из каждого элемента\n",
        "  # и с помощью генерации списков мы все это сохраняем в хранилище content\n",
        "  content.append([get_content(c) for c in contents])"
      ],
      "metadata": {
        "id": "oUIXsBJRuum1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# сохраним наш корпус в отдельный файл\n",
        "with open(\"corpus.json\", \"w\", encoding=\"utf8\") as f:\n",
        "   json.dump(content, f, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "lLXnQZBWmiOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание:\n",
        "\n",
        "- создать функцию, которая принимает на вход даты и возвращает корпус\n",
        "- функция должна line-by-line сохранять все в файл\n",
        "- \"поиграть\" с разметкой корпуса\n",
        "- сохранить в другом формате (csv, txt)\n",
        "- собрать корпус для другого издания\n",
        "- собрать более крупный корпус"
      ],
      "metadata": {
        "id": "FyT4-l1B5JHe"
      }
    }
  ]
}
